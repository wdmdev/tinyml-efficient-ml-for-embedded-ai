{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TinyML: Efficient Machine Learning for Embedded AI Systems","text":"<p>(Danish Title) TinyML: Effektiv maskinl\u00e6ring til indlejrede AI-systemer.</p> <p>My way of creating an overview of a special course I am taking at The Technical University of Denmark as  part of my MSc in Mathematical Modeling and Computation with a Machine Learning and Signal Processing specialization.  </p> <p>This page is created as a fork of the following course page Real-Time Visual and Machine Learning Systems. Thanks to Anders Bo S\u00f8rensen and his co-workers for making it available!  Lightning icons created by Freepik - Flaticon</p> <p>See the actual site here: TinyML: Efficient Machine Learning for Embedded Ai Systems .</p>"},{"location":"#course-description","title":"Course Description","text":"<p>This course provides an introduction to TinyML, focusing on efficient machine learning for embedded AI systems. Students will learn the fundamental concepts and techniques for developing machine learning models that can run on resource-constrained devices. Topics covered include model optimization, quantization, neural architecture search, knowledge distillation, and domain-specific optimization. The course will also explore efficient training methods, distributed training, on-device training, and transfer learning. Hands-on labs and readings will complement the lectures, allowing students to gain practical experience in implementing TinyML systems. By the end of the course, students will have the knowledge and skills to design and deploy machine learning models on embedded devices.</p>"},{"location":"#learning-objectives","title":"Learning Objectives","text":"<p>A student who has met the objectives of the course will be able to:</p> <ul> <li>Understand the concept of TinyML and its applications in embedded AI systems.</li> <li>Learn techniques for optimizing machine learning models for resource-constrained devices.</li> <li>Gain knowledge in model quantization, neural architecture search, knowledge distillation, and domain-specific optimization.</li> <li>Explore efficient training methods, including distributed training, on-device training, and transfer learning.</li> <li>Develop practical skills in implementing TinyML systems through hands-on labs.</li> <li>Design and deploy machine learning models on embedded devices.</li> </ul>"},{"location":"#evaluation","title":"Evaluation","text":"<ul> <li>Summary report of the learnings throughout the course</li> </ul>"},{"location":"#course-work","title":"Course work","text":"<p>The course will consist of:</p> <ul> <li>Lectures</li> <li>Some readings (mainly papers)</li> <li>Hands-on labs</li> </ul> <p>For extra resources one may find the following useful:</p> <ul> <li>Machine Learning Systems</li> <li>Paper Overview - CMU On-Device Machine Learning</li> </ul>"},{"location":"#schedule","title":"Schedule","text":"<p>The course starts on Friday 10 June and runs until Friday 28 June 2024.</p> Date Labs Topics Lecture Efficient Inference 6/10/24 Lab 1 Pruning and Sparsity Part 1 , Part 2 6/11/24 Lab 2 Quantization Part 1, Part 2 6/12/24 Lab 3 Neural Architecture Search Part 1, Part 2 6/13/24 Knowledge Distillation Link 6/14/24 Frameworks MCUNet, TinyEngine Domain Specific Optimization 6/17/24 Lab 4 and Lab 5 Transformer and LLM Part 1, Part 2 6/18/24 Lab 6 Vision Transformer Link 6/19/24 GAN, Video, and Point Cloud Link 6/20/24 Diffusion Model Link 6/21/24 Recurrent Neural Networks Paper Efficient Training 6/24/24 Distributed Training Part 1, Part 2 6/25/24 On-Device Training and Transfer Learning Link 6/26/24 Efficient Fine-tuning and Prompt Engineering Link 6/27/24 Lab 7 Benchmarking TinyML Systems Paper 6/28/24 Lab 8 Carbon and Energy Tracking Paper"}]}